{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install IPython \n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "HYQB4ZCwoC8U"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai \n",
        "!pip install wget\n",
        "!pip install pathlib \n",
        "!pip install pdfplumber\n",
        "!pip install fastai \n",
        "!pip install convertapi\n",
        "clear_output() "
      ],
      "metadata": {
        "id": "Zk_dUJDvdSkU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgdZ2BQDJuhp",
        "outputId": "5c1aa17d-6d3e-468f-9abc-6a0f39365c22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/drive/MyDrive'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVcNFDPVJw2s",
        "outputId": "c4f43f89-7b89-4b9f-e7bc-74fcc8adbc95"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap \n",
        "import openai \n",
        "import os\n",
        "import time \n",
        "import convertapi\n",
        "\n",
        "openai.api_key='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'\n",
        "\n",
        "def gpt3(stext):\n",
        "    openai.api_key='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'\n",
        "    response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=stext+\"\\n\\nTl;dr\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=60,\n",
        "    top_p=1.0,\n",
        "    frequency_penalty=0.0,\n",
        "    presence_penalty=1\n",
        "    )\n",
        "    content=response.choices[0].text\n",
        "    print(content)\n",
        "    return response.choices[0].text\n",
        "    \n",
        "    \n",
        "def open_file(filepath):\n",
        "    with open(filepath,'r',encoding='utf-8') as infile:\n",
        "        return infile.read()\n",
        "\n",
        "def save_file(content,filepath):\n",
        "    with open(filepath,'w',encoding='utf-8') as outfile:\n",
        "        outfile.write(content)\n",
        "\n",
        "def convert_pdf_to_text():\n",
        "    file_path_pdf=input(\"paste the pdf link of the paper (from arxiv or from your drive): \")\n",
        "    convertapi.api_secret = 'osUD4WoPonwG3eKX'\n",
        "    txt_address=convertapi.convert('txt', {'File': file_path_pdf}, from_format = 'pdf').save_files('/content/drive/MyDrive')\n",
        "    return txt_address[0]\n",
        "\n",
        "\n",
        "def generate_final_summary():\n",
        "    alltext=open_file(convert_pdf_to_text())\n",
        "    chunks=textwrap.wrap(alltext,2000)\n",
        "    result=list()\n",
        "    for chunk in chunks:\n",
        "        # prompt=open_file('/content/drive/MyDrive/prompt.txt').replace('<<SUMMARY>',chunk)\n",
        "        result.append(gpt3(chunk))\n",
        "    return result\n",
        "\n",
        "def generate_multiple_summary(alltext):\n",
        "    chunks=textwrap.wrap(alltext,2000)\n",
        "    result=list()\n",
        "    for chunk in chunks:\n",
        "        # prompt=open_file('/content/drive/MyDrive/prompt.txt').replace('<<SUMMARY>',chunk)\n",
        "        result.append(gpt3(chunk))\n",
        "    return result\n",
        "\n",
        "def full_summary_generation():\n",
        "    #for multiple papers\n",
        "    number_of_papers=int(input(\"enter how many papers you want to summarize: \"))\n",
        "    final_summary=[]\n",
        "    individual_summaries=[]\n",
        "\n",
        "    for i in range(number_of_papers):\n",
        "        individual_summaries=generate_final_summary()\n",
        "    prefinal_summary=\"\"\n",
        "\n",
        "    for summary in individual_summaries:\n",
        "        prefinal_summary+= summary\n",
        "\n",
        "    final_summary=generate_multiple_summary(prefinal_summary)\n",
        "    return final_summary \n"
      ],
      "metadata": {
        "id": "IDRjbpRVgWaY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_summary_generation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERHNBd3WiwX1",
        "outputId": "18514dca-d765-4784-d33b-81642129d767"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter how many papers you want to summarize: 3\n",
            "paste the pdf link of the paper (from arxiv or from your drive): https://arxiv.org/pdf/2301.01236.pdf\n",
            ": This tutorial introduces parametric variational inference, a method of approximate Bayesian inference that uses optimization rather than integration to approximate the marginal likelihood and posterior in a Bayesian model. Variational inference is computationally efficient and scalable to high-dimensional models and large datasets, making it an ideal choice for\n",
            "\n",
            "\n",
            "Variational Inference is a method of approximating the posterior distribution of latent variables from observed data. It involves using a tractable lower bound on the marginal likelihood, called the Evidence Lower Bound (ELBO), to replace the intractable true posterior. This lower bound is maximized through\n",
            "\n",
            "\n",
            "We can estimate the marginal likelihood and circumvent the need to evaluate the posterior by using two mathematical identities, rewriting the marginal likelihood as an evidence lower bound (ELBO), and minimizing the KL divergence. As an example, we can use this method to calculate the marginal likelihood of a single Exp(\n",
            ": The conjugate prior for ß is a Gamma distribution, and the posterior is also a Gamma distribution. Using variational inference, we can fit a Lognormal(µ, σ2) distribution to the posterior by maximizing the ELBO. We can use \"standard\" optimization\n",
            ": Parametric variational inference is a powerful tool for approximate Bayesian inference. We can maximize the evidence lower bound (ELBO) by parameterizing the variational posterior with respect to parameters θ and then optimizing these parameters using gradient-based methods such as gradient descent. The key requirement is that\n",
            "\n",
            "\n",
            "This tutorial provides an introduction to parametric variational inference, a powerful tool for approximating difficult integrals in Bayesian inference. It begins by discussing the basic principles behind variational inference, then moves on to discuss how to use it to optimize an approximation to the log-evidence of a\n",
            ": The reparameterization trick is a method of decoupling the source of randomness from the parameters by reformulating the random variable z~qθ(z) as a parameterized transformation s=gθ(c) of another random variable c~p(c). This\n",
            ": This paper gives an overview of parametric variational inference, which is a method for approximating the posterior distribution of a probabilistic model. It introduces several examples of how to calculate the ELBO and its partial derivatives. Finally, it discusses amortized variational inference and how it can\n",
            ":\n",
            "The ELBO maximization in equation 10 decomposes into a sum of local ELBOs. This led to the idea of amortized variational inference, wherein a machine learning model (often a neural network) is trained to directly predict the solution θ× of this optimization problem.\n",
            "\n",
            "\n",
            "This paper provides a tutorial on the basics of variational inference, which is a popular technique for approximate Bayesian inference. It covers the basic concepts, such as the goal of variational inference and the ELBO, and introduces two popular variations: mean-field variational inference and black box\n",
            ":\n",
            "This paper provides an overview of variational inference, its uses, and how to implement it. Variational inference is a method of approximating the posterior distribution of a Bayesian model using optimization techniques. It has become increasingly popular in machine learning due to its ability to approximate complex distributions quickly and\n",
            "paste the pdf link of the paper (from arxiv or from your drive): https://arxiv.org/pdf/2301.04991.pdf\n",
            "- This research studied software developers' experience using the Github Copilot tool in the software development process. A survey containing 18 questions was prepared and distributed to programmers. The results indicated that most of the participants had met the tool before the survey, but their opinions were divided. Most had a positive attitude towards\n",
            "\n",
            "This paper examines the experience and attitude of software developers towards Github Copilot, an AI-based code generator released in June 2021. 42 responses from developers with different levels of experience and specializations were analyzed. The survey focused on developers' attitudes toward Github Copilot, their predictions about its future,\n",
            ": Dominik Sobania, Martin Briesch and Franz Rothlau compared Github Copilot to genetic programming in their work \"Choose Your Programming Copilot”, showing similar performance of both tools. Raphael Jenni discussed machine learning for programming languages and mentioned Github Copilot as one of the modern\n",
            ": This paper presents a survey on the security of Github Copilot, a tool that uses models trained on open source Github repositories to generate code. The survey was conducted among software developers and results show that while most participants were aware of the tool, they also expressed concern about its security. The authors also\n",
            ":\n",
            "\n",
            "The survey revealed that 66.7% of programmers had heard about GitHub Copilot, and 14.3% were using it. Of the 6 people who were using it, 50% had been doing so for more than two weeks. 5 of them were testing it, 2 were using\n",
            "\n",
            "\n",
            "This survey shows the attitude of developers towards Artificial Intelligence tools. The results indicate that 26 (61.9%) did not want to test it, while 16 (38.1%) did. Most participants had a positive attitude towards AI tools, with 14 (33.4%) using or wanting\n",
            "\n",
            "\n",
            "GitHub Copilot was evaluated by 40 developers in terms of helpfulness, impact on future employment and security. Most respondents found it helpful (87%), thought that it would increase their chances for employment (90%) and trusted the authors’ information about safety (59.5%). Before\n",
            "\n",
            "This research studied software developers’ experience and opinion about Github Copilot in the software development process. Most of the developers had heard about the tool, although it was relatively new. Most respondents did not use it, but after getting familiar with it, many would like to test it and believe that\n",
            ": This paper explored software developers' attitudes toward the new AI-pair programming tool, GitHub Copilot. Our survey results showed that opinions about this type of tool vary among developers. Most had heard of GitHub Copilot before attending the survey but only 14.3% had used it. Most applications referred\n",
            "\n",
            "\n",
            "A survey conducted among programmers to evaluate their opinion on Github Copilot’s security levels shows that most of the respondents specialize in Backend programming. While there are some concerns about data sent into projects and data leak from projects, even after reading the authors' assurance that data are sent safely\n",
            "paste the pdf link of the paper (from arxiv or from your drive): https://arxiv.org/pdf/2301.04257.pdf\n",
            "\n",
            "This paper proposes ODIM, an efficient method to identify outliers in training data with no label information. It exploits the inlier-memorization (IM) effect of deep generative models, which memorize inliers before outliers. The ODIM is computationally efficient and filters\n",
            ":\n",
            "This paper proposes a novel algorithm for unsupervised outlier detection (UOD) that uses the memorization effect observed in noisy label problems. Our algorithm is motivated by the fact that correctly labeled data are learned earlier and mislabeled data are learned later in the training phase of deep neural\n",
            ": We developed a new method, ODIM, to detect outliers using the inlier-memorization effect of deep generative models. This is a domain-agnostic UOD solver which does not require any prior expertise in data and does not require class labels, thus domain-specific\n",
            ":\n",
            "\n",
            "We propose a simple yet powerful outlier detection (OD) method called ODIM that is domain-agnostic and utilizes the IM effect. Our approach trains a deep generative model with a log-likelihood-based approach like VAE or IWAE, and then selects data\n",
            "\n",
            "This paper introduces ODIM, a new and efficient method for outlier detection in unlabeled data sets. The proposed algorithm leverages the Inlier-Memorization effect of deep generative models to accurately identify outliers. Experiments on various benchmark data sets demonstrate that ODIM outperforms\n",
            ":\n",
            "Various approaches for outlier detection, both supervised and unsupervised, have been developed using deep learning models. Supervised approaches such as DeepSVDD, Deep-SAD, and SimCLR all utilize self-supervised learning by generating artificial labels automatically to obtain a desirable feature\n",
            "\n",
            "\n",
            "We propose a new method for unsupervised outlier detection (UOD) called E³-Outlier. This method trains a deep neural network by self-supervised learning and identifies outliers based on how fast the loss decreases as the training proceeds. We explain the main motivation\n",
            "\n",
            "We provide a theoretical explanation for the inlier-memorization effect of deep generative models, which is observed when training a model in the direction of memorizing inliers first at the beginning of the training phase. We use a toy example of a linear factor model trained with a Vari\n",
            ": Proposition 3.1 suggests that the initial update direction of a deep generative model is determined mainly by the inliers, leading to an imbalance in the memorization of inliers and outliers, which in turn results in an IM effect. The result can be confirmed through the per-\n",
            ": We propose a new algorithm called Outlier Detection Via Inlier-Memorization Effect (ODIM) to detect outliers in deep generative models. ODIM consists of two steps: training a deep generative model for a pre-specified number of updates and using the per-sample loss\n",
            " \n",
            "We propose ODIM, an efficient method to detect outliers via the inlier-memorization effect of deep generative models. We use a log-likelihood-based approach with a computable lower bound such as the Evidence Lower Bound (ELBO). To measure the performance of\n",
            "  We introduce an adaptive strategy to decide the optimal number of updates for the IWAE objective function. We devise a heuristic strategy to choose the number of updates by observing the IM effect which is the difference of the loss distributions between inliers and outliers. The results of an ablation\n",
            ":\n",
            "We propose ODIM, an efficient method to detect outliers in deep generative models that uses the inlier-memorization effect of training. At each update of the model, we assess the degree of bimodality of the per-sample loss distribution and select the optimal number\n",
            ":\n",
            "\n",
            "We present ODIM, a novel deep learning algorithm for outlier detection. ODIM uses Wasserstein distance to select the optimal number of updates and an ensemble strategy to improve and stabilize the results. We compare ODIM with several state-of-the-art outlier detection algorithms\n",
            ": We present ODIM, an efficient method for detecting outliers via inlier-memorization effect of deep generative models. We show the superiority of our method through extensive experiments on 20 data sets, covering tabular, image, and sequential types. Results demonstrate that ODIM outperforms other\n",
            "\n",
            "We analyze two synthetic image datasets (MNIST and FMNIST) as well as a real-world image dataset (WM-811K). We pre-process the two synthetic datasets by selecting a normal class and randomly drawing samples from the remaining data. For WM-811K,\n",
            ": We analyze text-type data from the Reuters-21578 dataset with a TFIDF transformer, taking five of the largest classes and randomly drawing 360 samples from each class. We compare our results to those of IF, OCSVM, LOF, DeepSVDD, and RSRA\n",
            ": We compare our ODIM method to three machine-learning-based methods (Isolation Forests, One-Class SVM, and Local Outlier Factor) and two deep-learning-based methods (DeepSVDD and RSRAE). We use a two hidden layered DNN architecture\n",
            "\n",
            "The ODIM outperforms other baselines in terms of the performance for identifying outliers in a given training data set. Results for tabular data show that the ODIM achieves the best scores most frequently on both AUC and AP, while the AP values of the other methods have large variations.\n",
            " AUC AP IF OCSVM LOF DeepSVDD RSRAE Ours 0.574 (0.063) 0.878 0.812 0.501 (0.033) 0.915 (0.022) 0.888 (0.043) 0\n",
            ":\n",
            "Our method is faster than deep learning methods, and similar or even faster than non-deep learning methods for data with large sample sizes. We conjecture that the high dimensionality of data might put off the occurrence of the IM effect, which can be addressed by accelerating the IM effect to overcome the\n",
            ": We investigated the effect of ensembling in ODIM, a deep generative model for detecting outliers. We varied the number of models and compared the running time and performance of our model with various competitors on several datasets. Our results showed that using an ensemble of models improved performance and decreased running\n",
            ": We evaluated the robustness of our ODIM method to the learning schedule and compared its performance on various tabular datasets. We found that the optimal number of models used in the ensemble varied based on the dataset, but that the performance was not sensitive to the number of models used unless it is too\n",
            "\n",
            "ODIM is an efficient method for detecting outliers in deep generative models via the inlier-memorization effect. We evaluated ODIM on fourteen datasets and found that it identified normal samples well from unseen data. We also compared two data pre-processing methods, min-max and standard\n",
            "\n",
            "The IM effect is not invariant to the normalization process, and so is the input norm. This can be explained by the fact that the initial parameters in the model considered in Proposition 3.1 are independent mean 0 random variables, and thus, data far from the origin are not explained well\n",
            ":\n",
            "ODIM is an efficient method for detecting outliers via the inlier-memorization effect of deep generative models. It has been found to provide consistently superior results regardless of data types with much faster running times. The ODIM utilizes a variational upper bound called CUBO that encourages\n",
            " \n",
            "This paper presents an efficient unsupervised outlier detection method called ODIM. The method is based on the observation that deep generative models tend to memorize inliers first when they are trained. It combines this observation with the technique to select the optimal number of updates, resulting in\n",
            ": Deep generative models can be used to detect outliers by using the inlier-memorization effect. This method is supported by Arjovsky et al., Arpit et al., Bergman and Hoshen, Breunig et al., Burda et al., Chalapathy\n",
            "\n",
            "Deep learning has been used in a variety of tasks, such as contrastive learning of visual representations, deep variational semi-supervised novelty detection, pre-training of deep bidirectional transformers for language understanding, deep anomaly detection using geometric transformations, an information geometry approach to out-of\n",
            ": Jiang et al. (2017, 2022) proposed flow generative models for out-of-distribution detection, while Kim et al. (2020) used importance weighted autoencoders to learn deep generative models. Lai et al. (2020a, 2020b) developed a\n",
            "\n",
            "\n",
            "This paper reviews various out-of-distribution detection methods, including Isolation Forest, Unsupervised One-Class Learning for Automatic Outlier Removal, Multiscale Score Matching for Out-of-Distribution Detection, Detecting Out-of-Distribution Inputs to Deep\n",
            ": ODIM is a new method for unsupervised outlier detection, which uses deep generative models to memorize inliers and detect outliers. It is an efficient method that can be used with both labeled and unlabeled data. ODIM has been shown to outperform existing methods\n",
            "\n",
            "\n",
            "ODIM is an efficient method for unsupervised outlier detection which utilizes the inlier-memorization effect of deep generative models. It can be used to detect anomalous data points from large datasets and is useful for tasks such as failure pattern recognition and anomaly detection.\n",
            ": The objective function of the VAE can be characterized in terms of its derivatives with respect to the elements of W and b. Specifically, the derivative w.r.t. wij is given by an expression involving the parameters U, V, and η. This expression can then be squared\n",
            " The expected value of the log probability with respect to θ and φ is equal to 1 times the sum of (x - b)² + the sum of w² (u′ x + v)² + 2wijwij' (u′jx + vj)(u\n",
            "\n",
            "ODIM is an efficient method to detect outliers via inlier-memorization effect of deep generative models. The results of the ODIM with various values of K in table A.1 show that it is effective in detecting outliers in datasets such as BreastW, Glass, Ion\n",
            ":\n",
            "Table A.2 compares the running times of IF, OCSVM, LOF, DeepSVDD, RSRAE and our method on various datasets. Table A.3 shows the AUC results of ODIM with various number of generative models to take an ensemble.\n",
            ":\n",
            "The average accuracy of all the datasets tested using XGBoost is around 0.9, with some datasets achieving higher scores (Mammography - 0.86, Pendigits - 0.96 and Satimage-2 - 0.99). Arrhythmia and Cardio\n",
            ": We present a new method for unsupervised outlier detection (UOD) called ODIM, which uses the Inlier-Memorization effect of deep generative models to accurately identify outliers. Experiments on various benchmark data sets demonstrate that ODIM outperforms existing methods, making it\n",
            ": We propose ODIM, an efficient method to detect outliers in deep generative models by leveraging the inlier-memorization effect of training. We use a log-likelihood-based approach with a computable lower bound such as the Evidence Lower Bound (ELBO) to measure the\n",
            ": We evaluated the performance of our ODIM method for detecting outliers in multiple datasets compared to other machine-learning and deep-learning-based methods. Results show that ODIM outperforms all competitors in terms of AUC and AP scores, and is faster than deep learning methods with large sample sizes\n",
            ": ODIM is an efficient unsupervised outlier detection method that utilizes the inlier-memorization effect of deep generative models. It has been found to provide consistently superior results regardless of data types with much faster running times. The ODIM combines this observation with a technique to select the\n",
            ": ODIM is an efficient method for unsupervised outlier detection which utilizes the inlier-memorization effect of deep generative models. It can be used to detect anomalous data points from large datasets, and has been shown to outperform existing methods such as Isolation Forest, Un\n",
            ": XGBoost achieved an accuracy score of around 0.9 on most datasets tested, with some datasets achieving higher scores such as Mammography (0.86), Pendigits (0.96) and Satimage-2 (0.99). Arrhythmia and Cardio had\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[': We present a new method for unsupervised outlier detection (UOD) called ODIM, which uses the Inlier-Memorization effect of deep generative models to accurately identify outliers. Experiments on various benchmark data sets demonstrate that ODIM outperforms existing methods, making it',\n",
              " ': We propose ODIM, an efficient method to detect outliers in deep generative models by leveraging the inlier-memorization effect of training. We use a log-likelihood-based approach with a computable lower bound such as the Evidence Lower Bound (ELBO) to measure the',\n",
              " ': We evaluated the performance of our ODIM method for detecting outliers in multiple datasets compared to other machine-learning and deep-learning-based methods. Results show that ODIM outperforms all competitors in terms of AUC and AP scores, and is faster than deep learning methods with large sample sizes',\n",
              " ': ODIM is an efficient unsupervised outlier detection method that utilizes the inlier-memorization effect of deep generative models. It has been found to provide consistently superior results regardless of data types with much faster running times. The ODIM combines this observation with a technique to select the',\n",
              " ': ODIM is an efficient method for unsupervised outlier detection which utilizes the inlier-memorization effect of deep generative models. It can be used to detect anomalous data points from large datasets, and has been shown to outperform existing methods such as Isolation Forest, Un',\n",
              " ': XGBoost achieved an accuracy score of around 0.9 on most datasets tested, with some datasets achieving higher scores such as Mammography (0.86), Pendigits (0.96) and Satimage-2 (0.99). Arrhythmia and Cardio had']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}